{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7692d61-9085-4841-9143-c339427e2fc8",
   "metadata": {},
   "source": [
    "# Pretrained GPT2 Model to generate Elon Musk Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec6594-6474-4757-9951-7b8514a35f13",
   "metadata": {},
   "source": [
    "**Training Data:**   \n",
    "Source: https://www.kaggle.com/datasets/aryansingh0909/elon-musk-tweets-updated-daily?select=elonmusk.csv   \n",
    "License: CC0: Public Domain   \n",
    "\n",
    "Enviroment: tf-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edc7cf5-317c-42d5-81b0-108082e2e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Conny\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee13e2bc-90ad-4724-8822-2e810b081550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e46649ce-a495-4eb9-960d-f2fdd1c6bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c20b4-2553-4d8f-8346-0f6cf53db18f",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb882a9-ded3-489b-b467-f09f60d143b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-29 16:20:19+00:00</td>\n",
       "      <td>1674452749378002945</td>\n",
       "      <td>@mwseibel Yup</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-29 15:53:52+00:00</td>\n",
       "      <td>1674446089179766789</td>\n",
       "      <td>@TitterDaily True</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-29 03:46:37+00:00</td>\n",
       "      <td>1674263071337111552</td>\n",
       "      <td>@paulg Generational trauma. An example of why ...</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-29 01:19:59+00:00</td>\n",
       "      <td>1674226170488057856</td>\n",
       "      <td>Improved longform posts</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-28 18:05:58+00:00</td>\n",
       "      <td>1674116945808068608</td>\n",
       "      <td>@BillyM2k Best protip ever</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24445</th>\n",
       "      <td>2011-12-03 08:22:07+00:00</td>\n",
       "      <td>142881284019060736</td>\n",
       "      <td>That was a total non sequitur btw</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24446</th>\n",
       "      <td>2011-12-03 08:20:28+00:00</td>\n",
       "      <td>142880871391838208</td>\n",
       "      <td>Great Voltaire quote, arguably better than Twa...</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24447</th>\n",
       "      <td>2011-12-01 10:29:04+00:00</td>\n",
       "      <td>142188458125963264</td>\n",
       "      <td>I made the volume on the Model S http://t.co/w...</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24448</th>\n",
       "      <td>2011-12-01 09:55:11+00:00</td>\n",
       "      <td>142179928203460608</td>\n",
       "      <td>Went to Iceland on Sat to ride bumper cars on ...</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24449</th>\n",
       "      <td>2010-06-04 18:31:57+00:00</td>\n",
       "      <td>15434727182</td>\n",
       "      <td>Please ignore prior tweets, as that was someon...</td>\n",
       "      <td>elonmusk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24450 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Datetime             Tweet Id  \\\n",
       "0      2023-06-29 16:20:19+00:00  1674452749378002945   \n",
       "1      2023-06-29 15:53:52+00:00  1674446089179766789   \n",
       "2      2023-06-29 03:46:37+00:00  1674263071337111552   \n",
       "3      2023-06-29 01:19:59+00:00  1674226170488057856   \n",
       "4      2023-06-28 18:05:58+00:00  1674116945808068608   \n",
       "...                          ...                  ...   \n",
       "24445  2011-12-03 08:22:07+00:00   142881284019060736   \n",
       "24446  2011-12-03 08:20:28+00:00   142880871391838208   \n",
       "24447  2011-12-01 10:29:04+00:00   142188458125963264   \n",
       "24448  2011-12-01 09:55:11+00:00   142179928203460608   \n",
       "24449  2010-06-04 18:31:57+00:00          15434727182   \n",
       "\n",
       "                                                    Text  Username  \n",
       "0                                          @mwseibel Yup  elonmusk  \n",
       "1                                      @TitterDaily True  elonmusk  \n",
       "2      @paulg Generational trauma. An example of why ...  elonmusk  \n",
       "3                                Improved longform posts  elonmusk  \n",
       "4                             @BillyM2k Best protip ever  elonmusk  \n",
       "...                                                  ...       ...  \n",
       "24445                  That was a total non sequitur btw  elonmusk  \n",
       "24446  Great Voltaire quote, arguably better than Twa...  elonmusk  \n",
       "24447  I made the volume on the Model S http://t.co/w...  elonmusk  \n",
       "24448  Went to Iceland on Sat to ride bumper cars on ...  elonmusk  \n",
       "24449  Please ignore prior tweets, as that was someon...  elonmusk  \n",
       "\n",
       "[24450 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('./data/elonmusk.csv')\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079b785-991c-4c3c-8a46-d17fd4e7b753",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84cdf294-501b-47ec-9390-19ea501627f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Datetime             Tweet Id  \\\n",
      "2   2023-06-29 03:46:37+00:00  1674263071337111552   \n",
      "10  2023-06-28 02:53:54+00:00  1673887418184007680   \n",
      "17  2023-06-27 23:12:47+00:00  1673831774038949888   \n",
      "19  2023-06-27 16:14:14+00:00  1673726440284495872   \n",
      "21  2023-06-27 13:27:15+00:00  1673684418974609410   \n",
      "\n",
      "                                                 Text  Username  \n",
      "2   @paulg Generational trauma. An example of why ...  elonmusk  \n",
      "10  @cb_doge @TuckerCarlson @TheBabylonBee @ZubyMu...  elonmusk  \n",
      "17  Watch the entire first episode on this platfor...  elonmusk  \n",
      "19  @RobertMSterling @WholeMarsBlog @nytimes Prett...  elonmusk  \n",
      "21  @InfographicTony @MarcusHouse @FelixSchlang @c...  elonmusk  \n"
     ]
    }
   ],
   "source": [
    "# sort out short tweets <7 words\n",
    "min_word_count = 7\n",
    "tweets_df['word_count'] = tweets_df['Text'].apply(lambda x: len(x.split()))\n",
    "df_filtered = tweets_df[tweets_df['word_count'] >= min_word_count]\n",
    "\n",
    "# extract text column\n",
    "texts = df_filtered['Text'].tolist()\n",
    "\n",
    "# extract word_count column\n",
    "df_filtered = df_filtered.drop(columns=['word_count'])\n",
    "\n",
    "print(df_filtered.head())# sort out short tweets <7 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ceb13c6-efb7-4ae4-b369-2c93acf42f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@paulg Generational trauma. An example of why forgiveness of those who harmed you (if it stops further harm) is so important.\n",
      "@cb_doge @TuckerCarlson @TheBabylonBee @ZubyMusic @AppleTV Very exciting!\n",
      "Watch the entire first episode on this platform. Great move by Apple!\n",
      "\n",
      "Note, you can Airplay from your iPhone to TV to watch on a big screen.\n",
      "@RobertMSterling @WholeMarsBlog @nytimes Pretty much only time I see NYT articles is when they’re mentioned here. Their readership, especially user-minutes per day, is tiny compared to this platform.\n",
      "@InfographicTony @MarcusHouse @FelixSchlang @considercosmos @GregScott_photo @LunarCaveman @SpaceX @SpacesFuture @TJ_Cooney @LabPadre @Erdayastronaut Three center Raptors of Booster will fire at ~50% thrust during hot staging\n"
     ]
    }
   ],
   "source": [
    "# create a list of tweet texts\n",
    "texts = df_filtered['Text'].tolist()\n",
    "\n",
    "for text in texts[:5]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4ca9db-a9bc-47ff-8505-ae9f9a5e69de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generational trauma. an example forgiveness harmed if stops harm important.',\n",
       " 'very exciting!',\n",
       " 'watch entire first episode platform. great move apple! note airplay iphone tv watch big screen.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning and preprocessing of the tweet texts\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    text = re.sub(r'@\\w+', '', text)  # Removal of mentions\n",
    "    text = re.sub(r'http\\S+', '', text)  # Removal of URLs\n",
    "    text = re.sub(r'[^\\w\\s.!?]', '', text)  # Removal of only certain punctuation marks\n",
    "    return text.strip().lower() #Removal of spaces at the beginning and end of strings and conversion of the entire string to lower case letters\n",
    "\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "cleaned_texts[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c09cf-738e-4f69-89d6-7e9a4995abf3",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c882e4-9dde-4fc9-9562-bf4c635849eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Conny\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\modeling_utils.py:367: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "100%|██████████| 14/14 [00:01<00:00,  7.52ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Conny\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13960\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30/872 04:26 < 2:13:43, 0.10 it/s, Epoch 0.07/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dataset from the tweets\n",
    "dataset = Dataset.from_dict({'text': cleaned_texts})\n",
    "\n",
    "# Load model und tokenizer\n",
    "model_name = 'distilgpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Specify pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokenized_output = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=50)\n",
    "    tokenized_output['labels'] = tokenized_output['input_ids'].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,  # Number of training epochs\n",
    "    per_device_train_batch_size=8, # Batch Size\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
    "    save_steps=10_000, # Save model every 10.000 steps\n",
    "    save_total_limit=2,  \n",
    "    learning_rate=5e-5, # Low learning rate to find the optimum parameters\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "# Model training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3cee6-6093-45a0-89d9-29a3c18f04eb",
   "metadata": {},
   "source": [
    "## Tweet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b921b36-49e9-4f41-b47b-3fb5ca0ef269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweet(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example: Generation of a tweet based on a start word or phrase\n",
    "prompt = \"In the future Tesla will\"\n",
    "generated_tweet = generate_tweet(prompt)\n",
    "print(generated_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce1725-d20f-4076-8a44-75716dad6c42",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befab5e1-3894-4a6b-b3a0-a59a2381f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb1463-db7d-49fe-814b-66e70f09c46a",
   "metadata": {},
   "source": [
    "## Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7b4ec-04f6-4bd6-93bd-b1a4c4b2eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./saved_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./saved_model\")\n",
    "\n",
    "# Example of the use of the model\n",
    "inputs = tokenizer(\"With the help of ai we will\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
